---
title: 'Análise de agrupamento aplicada aos resumos dos papers apresentados no evento SIGKDD2016'
academic: Eduardo Elias Ribeiro Junior
email: 'edujrrib@gmail.com'
chair: LCE5859 - Métodos Estatísticos Multivariados
institute: 'Escola Superior de Agricultura Luiz de Queiroz - USP'
date: '\today'
logo: "configs/logo-esalq.png"
bibliography: lce5859-mem.bib
csl: configs/abntcite.csl
header-includes:
  - \usepackage[hmargin={2.5cm, 2.5cm}, vmargin={3cm, 3cm}]{geometry}
  - \usepackage[portuguese, linesnumbered, vlined, ruled, commentsnumbered]{algorithm2e}
  - \SetAlCapSty{}
  - \SetAlCapFnt{\normalsize}
  - \SetAlCapNameFnt{\small}
output:
  bookdown::pdf_document2:
    template: configs/template.tex
    keep_tex: false
---

\begin{center}
\large Eduardo Elias Ribeiro Junior$^\ast$\footnote{Contato:
<edujrrib@gmail.com>/<jreduardo.usp.br>}\\
$^*$ Departamento de Ciências Exatas LCE ESALQ-USP
\end{center}
\vspace{0.5cm}

\begin{abstract}

Atualmente com o avanço no poder de armazenagem muitos conhecimento é
produzido e mantido em diversas mídias (servidores, unidades de disco,
pendrives, etc.). Na área acadêmica o produto relacionado à transmissão
de conhecimento são artigos. Comumente, em eventos científicos, têm-se
os artigos expostos que são agrupados em diferentes temas. Todavia esse
agrupamento é em geral realizado por humanos e nem sempre são
efetivos. Nesse artigo propõe-se uma metodologia para agrupamento de
artigos científicos, com base nos textos contidos em seus títulos e
resumos, baseados em técnicas estatísticas multivariadas. Os dados
utilizados no trabalhos são referentes aos títulos e resumos de 203
artigos apresentados na 22nd SIGKDD Conference e foram obtidos via
ferramentas de webscrapping. Após higienização dos textos foram obtidos
3173 termos distintos que foram organizados em uma matriz
termo-documento de frequências. Utilizando componentes principais as
3173 colunas da matriz termo-documento foram reduzidas a 199 componentes
principais. Com as componentes principais estimou-se, via estatísticas
Gap, que apenas 6 grupos seriam adequados para agrupamento, o que mostra
que a atribuição à 16 temas distintos feita pela organização do evento é
desnecessária. Os grupos puderam ser interpretados e a análise destes
mostrou que o agrupamento foi satisfatório. Todo o trabalho foi
elaborado com o auxílio do software R e os códigos estão disponíveis em
um material suplementar online.

\vspace{0.2cm}
\noindent
\textbf{Palavras-chave: }{\it text-mining, web-scrapping,
agrupamento, pca, k-means, estatísticas Gap}.

\end{abstract}

\pagebreak

```{r, include=FALSE}

##----------------------------------------------------------------------
## Reports
library(knitr)
library(xtable)
opts_chunk$set(
    warning = FALSE,
    message = FALSE,
    cache = FALSE,
    echo = FALSE,
    results = "hide",
    fig.width = 7,
    fig.height = 5,
    fig.align = "center",
    fig.pos = "H",
    out.width = "1\\textwidth",
    dev.args = list(
        family = "Palatino")
    )
options(
    digits = 3,
    xtable.comment = FALSE,
    xtable.caption.placement = "top",
    xtable.table.placement = "ht",
    xtable.sanitize.text.function = identity
)

##----------------------------------------------------------------------
## Packages

library(magrittr)
library(SnowballC)
library(tm)
library(clusterSim)

## For graphics
library(wordcloud)
library(lattice)
library(latticeExtra)
source("configs/setup.R")
cols <- trellis.par.get("superpose.line")$col

##----------------------------------------------------------------------
## Functions
text2dtm <- function(text) {
    text %>%
        VectorSource %>%
        Corpus %>%
        tm_map(removeWords,
               c(stopwords("english"),
                 "can")) %>%
        tm_map(removePunctuation) %>%
        tm_map(stripWhitespace) %>%
        tm_map(removeNumbers) %>%
        tm_map(stemDocument) %>%
        DocumentTermMatrix
}

```

\pagebreak

# Introdução #

Atualmente se produz muito conhecimento que, com o avanço da computação
e poder de armazenagem, estão mantidos em diversos mídias (servidores,
unidades de disco, pendrives, etc.) ao redor do mundo. Esse conhecimento
é transcrito em diferentes produtos como artigos científicos, páginas
web, blogs, vídeos entre outros. Todavia, quanto maior a quantidade de
conteúdo maior a dificuldade para sua organização.

Na tentativa de organizar esses produtos há diversos esforços voltados à
análise de texto, uma vez que, das mídias citadas as mais comuns são
expressas em forma de texto (como artigos, posts, livros,
etc.). Dificuldades para análise ou mineração de textos são frequentes e
estão presentes desde a coleta dos dados até a escolha da técnica
adequada para análise e interpretação dos resultados.

Na mineração de dados textuais pode-se listar duas estratégias
principais i) _Bag of words_ em que a semântica do texto é desfeita e a
estrutura linguística ignorada, sob essa abordagem os textos são
representados pela frequência ou ocorrência das palavras; e ii) _Natural
Processing Language (NLP)_ em que as palavras são caracterizadas pelo
seu sentido morfológico e os textos são analisados considerando
elementos da linguagem [@Berry2010]. A estratégia via _bag o words_,
embora simples, é extremamente útil e mais comum. Os principais
objetivos sob essa estratégia são agrupamento, classificação e predição
de textos [@Silge2017] embora outros possam existir.

Uma aplicação direta da análise de texto pode ser pensada para a
comunidade acadêmica. Comumente eventos da comunidade científica reúnem
pesquisadores para exposição e discussão de seus recentes
trabalhos. Esses trabalhos são submetidos ao evento e, em geral, são
classificados por temas, o que facilita i) os participantes a
localizarem seus interesses e ii) consultas ao acervo após finalização
do evento. Todavia, há pouco rigor na atribuição desses temas e muitas
vezes, pela má atribuição, esses acabam sendo dispensáveis. Assim a o
agrupamento de textos para geração de temas com trabalhos homogêneos tem
extrema relevância para científicos de eventos.

Nesse artigo apresentamos uma análise textual dos resumos dos artigos
apresentados na _22nd SIGKDD Conference_, maior evento de maior evento
de Knowledge Discovery and Data Mining promovido pela Association for
Computing Machinery (ACM), com o objetivo de agrupar os artigos pela
similaridade de seus textos. Isso é realizado via abordagem _bag of
words_ utilizando técnicas multivariadas para redução de
dimensionalidade e agrupamento. Após agrupamento os grupos são
interpretados como temas e contrastados com os temas criados pela
organização do evento.

O artigo é organizado em cinco seções. Essa primeira seção enfatiza a
importância da análise de textos e sua relevância em eventos
científicos. Na \autoref{conjunto-de-dados} o conjunto de textos é
descrito assim como destacado o procedimento de _web scrapping_ para sua
obtenção. Na \autoref{metodologia} são descritos os métodos utilizados
na análise dos dados e na \autoref{resultados-e-discussao} apresentados
os resultados da aplicação dos métodos bem como algumas discussões. Por
fim a \autoref{conclusoes} é destinada às considerações finais obtidas
desse trabalho e à apresentação de possíveis direções para pesquisas
futuras.

# Conjunto de dados #

```{r}

##----------------------------------------------------------------------
## Read and organize data

## Data long, each line is paper combine with topic (duplicate papers)
data_long <- read.table(
    file = "./data/kdd-papers.txt",
    header = TRUE,
    sep = ";",
    colClasses = c("factor", "character", "character")
)

## Data short, each line is unique paper
data_short <- data_long[!duplicated(data_long$title), -1]
data_short <- within(data_short, {
    ntopics = sapply(title, function(x) sum(x == data_long$title))
})

## Texts, using title and abstract texts
data_texts <- with(data_short, paste(title, abstract))

## Tranform to document-term matrix
dtm_texts <- text2dtm(data_texts)

```

O conjunto de dados analisados referem-se aos artigos apresentados na
22nd SIGKDD Conference realizada entre os dias 13 e 17 de agosto de 2016
sob organização da Association for Computing Machinery (ACM). As
informações sobre os artigos aceitos no SIGKDD estão disponíveis no
sítio eletrônico <http://www.kdd.org/kdd2016>. A \autoref{fig:webscrap}
ilustra a disposição das informações no sítio eletrônico do evento. Os
dados utilizados na análise correspondem aos textos dos títulos e
resumos dos artigos (destacados em vermelho na figura), além dos tópicos
atribuídos aos artigos pelos organizadores do evento (destacados em
azul).

Para extração dos dados utilizou-se as ferramentes para raspagem de
dados web disponíveis pelo pacote `rvest` [@pack-rvest] do
software R. O processo de extração se deu em três passos, devido a
disposição das informações conforme \autoref{fig:webscrap}:

1. Obtenção dos links para as páginas dos tópicos;
1. Obtenção dos links para as páginas dos artigos;
1. Obtenção das informação de título, resumo e tópicos para cada
   página de cada artigo.

```{r webscrap, out.width="0.8\\textwidth", results="asis", fig.cap="Sítio do SIGKDD2016 de onde foram extraídos os títulos, resumos e tópicos."}

## Web pages images
include_graphics("images/webscrap.pdf")

```

```{r}

##----------------------------------------------------------------------
## Descriptive analysis
mat_texts <- as.matrix(dtm_texts)
counts <- colSums(mat_texts)

## Frequency tables
freq_papers <- table(data_long$topic)
freq_topics <- table(paste(data_short$ntopics, "Tópico(s)"))

```

Ao todo foram `r nrow(data_long)` páginas consultadas, referentes à
`r nrow(data_short)` artigos. Na \autoref{tab:freq} são apresentadas os
`r nlevels(data_long$topic)` tópicos definidos pelos organizadores do
SIGKDD e o número de artigos pertencentes a cada respectivo tópico. Note
que nessa tabela a soma de artigos não representa o número de artigos
únicos apresentados no evento, pois mais de um tópico pode ser atribuído
ao mesmo artigo (veja a \autoref{fig:webscrap}). Observe também que há
uma moderada predominância de artigos cujo foram atribuídos os três
primeiros tópicos, ainda com apenas os 9 tópicos mais frequentes têm-se
aproximadamente 86\% de todas as atribuições. Das atribuições de tópicos
pelos organizadores foram `r freq_topics` artigos com 1, 2, 3, 4, 5 e 6
tópicos atribuídos, respectivamente

```{r, results="asis"}

aux <- sort(freq_papers, decreasing = TRUE)
tab_freq <- data.frame("Tópico" = names(aux),
                       "Nº de artigos" = c(aux),
                       "Freq. absoluta" = c(aux)/sum(c(aux)),
                       "Freq. acumulada" = cumsum(c(aux)/sum(c(aux))),
                       check.names = FALSE,
                       stringsAsFactors = FALSE)

tab_freq <- rbind(tab_freq, c(NA, colSums(tab_freq[, -1])[-3], NA))
tab_freq[17, 1] <- "\\textbf{Total}"
rownames(tab_freq) <- c(1:nlevels(data_long$topic), "")

## Build latex table
cap <- c("Frequência de artigos em cada tópico definido no evento.")
print(xtable(tab_freq, digits = c(0, 0, 0, 3, 3),
             align = c("llccc"),
             caption = cap,
             label = "tab:freq"))

```

# Metodologia #

Muito da análise de texto, não fundamentada em modelos estatísticos, é
realizada via técnicas multivariadas. Para atender o objetivo proposto
nesse trabalho faz-se uso de técnicas multivariadas de redução de
dimensionalidade via componentes principais (_PCA_) e de análise de
agrupamento via algoritmo de _k-means_. Nessa seção são apresentados os
métodos aplicados ao conjunto de textos a fim agrupá-los de forma
satisfatória.

## Organização de textos para análise ##

Uma importante e não trivial etapa no processo de análise de textos é a
adequação dos textos para análise. Conforme descrito na
\autoref{introducao} a abordagem via _bag of words_, transforma os
textos em uma matriz $\mathbb{X}$, cujo linhas são os documentos,
colunas são os termos e os valores representam as frequências dos termos
nos documentos. Para obtenção dos termos faz-se a higienização dos
textos que consiste na remoção das palavras de parada (preposições,
artigos, conjunções, etc.), da pontuação, dos espaços em branco e dos
números. Além da remoção desses caracteres, faz-se a radicalização das
palavras, para que palavras de mesmo sentido sejam associadas ao mesmo
termo, por exemplo as `clustering`, `cluster`, `clusterization`,
`clustered` possuem todas o mesmo radical, `cluster`, portanto ficam
todas associadas ao termo `cluster`. Existem vários algoritmos para
radicalização, nesse trabalho utilizou-se o algoritmo de Poter
[@porter2001].

## Redução de dimensionalidade via componentes principais ##

Não raramente a matriz termo-documento de frequências é retangular
possuindo um número de colunas (termos) muito maior que o número de
linhas (documentos), além de ser razoavelmente esparsa. Assim é de
interesse em análise de texto a redução do número de colunas da matriz
termo-documento $\mathbb{X}$. Uma técnica multivariada, cujo um dos
objetivos é a redução de dimensionalidade é chamada de análise de
componentes principais, que consiste construção de novas variáveis
denominadas componentes principais a partir da decomposição da matriz de
covariância ou correlação de $\mathbb{X}$ [@Friedman2001].

Para apresentação do método tome $\mathbf{R}$ como a matriz de
correlações de $\mathbb{X}$, de dimensão $p\times p$. $\mathbf{R}$ pode
ser escrita como $\mathbf{E} \Lambda \mathbf{E}^t$ ($\mathbf{R} =
\mathbf{E} \Lambda \mathbf{E}^t$), em que $\Lambda$ é a matriz diagonal
dos autovalores $\lambda_i$, $i=1,\ldots ,p$ da matriz $\mathbb{X}$, ou
seja, $\Lambda=\text{diag}(\underline{\lambda})$; e $\mathbf{E}$ é a
matriz dos autovetores $\underline{e}_i$, $i=1,\ldots ,p$. Assim as
novas variáveis, denominadas componentes principais são dadas por
$\underline{z}_i=\mathbb{X}\underline{e}_i$, para $i=1, \ldots, p$.

Note que são calculadas tantas componentes quanto for o número de
variáveis, porém apenas um subconjunto dessas componentes já é
suficiente para explicar boa parte da variação total dos dados originais
$\mathbb{X}$. O percentual da variância explicada pela i-ésima
componente é dada por $\lambda_i/\sum_{j=1}^p \lambda_j$.

A redução de dimensionalidade ocorre por tomar a matriz de componentes
principais $\mathbf{Z}$, com $m$ componentes, necessárias para
explicar um percentual arbitrário da variação total, $m<<p$, ao invés da
matriz $\mathbb{X}$. Embora @Manly2008 sugira que a análise de
agrupamento realizada sobre componentes principais deva ser evitada, em
problemas _"small n large p"_ essa é uma solução comumente utilizada.

## Agrupamento K-means ##

Métodos de agrupamento visam agrupar as observações de um conjunto de
dados em grupos, cujo as observação de um grupo sejam similares e as
observações de grupos distintos sejam distintas. Existem métodos de
agrupamentos hierárquicos e não-hierárquicos. Nos hierárquicos
normalmente se define o número de grupos a partir de um dendrograma,
essa escolha é geralmente subjetiva e priorizasse, nesses casos, a
interpretação dos grupos formados. Para os não-hierárquicos o número de
grupos deve ser informado previamente e então um algoritmo atribuirá as
observação aos grupos.


O método não-hierárquico mais comumente utilizado é o chamado método de
k-means [@Hartigan1979]. Nesse trabalho utiliza-se método de agrupamento
não-hierárquico supra-citado. O \autoref{alg:kmeans} descreve o
procedimento para formação dos grupos via _k-means_. Note que nos dados
de entrada do algoritmo o número de grupos a serem formados e os
dados devem ser especificados.

\begin{algorithm}[H]
  \caption{Agrupamento não-hierárquico de k-means.}
  \label{alg:kmeans}
  \SetAlgoLined
  \Entrada{\\
    \hspace{0.5cm}$k:$ número de grupos a serem formados (escalar
    inteiro);\\
    \hspace{0.5cm}$\mathbf{X}:$ conjunto de dados (matriz $n\times
    p$).\\
  }
  \Saida{\\
    \hspace{0.5cm}$\underline{g}:$ a atribuição dos grupos a cada
    observação (vetor de tamanho $n$);\\
  }
  \BlankLine
  \BlankLine
  Atribua aleatoriamente as observações aos grupos;\\
  \While{As atribuições dos grupos não for a mesma.}{\label{line:while}
    \textbf{(a)} Para cada grupo $k$ calcule o seu centroide. O
    centroide do $k$-ésimo grupo é dado pelo vetor média das $p$
    variáveis calculado das observação pertencentes a esse grupo;\\
    \textbf{(b)} Atribua cada observação ao grupo, cujo tem o centroide
     mais próximo, a proximidade é dada distância euclidiana entre o
     centroide e o vetor de variáveis da observação
  }
\end{algorithm}
\vspace{0.5cm}

Conforme visto no \autoref{alg:kmeans} o número de grupos $k$ deve ser
informado a priori, para avaliação do agrupamento realizado com $k$
grupos pode-se verificar as medidas de homogeneidade dos grupos e
heterogeneidade entre grupos. A homogeneidade é dada pelo inverso da
soma de quadrados das observações alocadas em um mesmo grupos e a
heterogeneidade pela soma de quadrado das observações alocadas em grupos
distintos.

O principal problema em análise de agrupamento é a escolha do número de
grupos, $k$, a serem formados. @Tibshirani2001 propuseram a estatística
Gap para estimação de $k$. Essa estatística é calculada usando as
medidas de homogeneidade do agrupamento em contraste com medidas de
variáveis simuladas, em geral, por uma distribuição uniforme. A escolha
do número de grupos é então realizada comparando as estatística Gap para
um $k$. Para maiores informações sobre o algoritmo consulte
@Tibshirani2001 e @clusterSim2016.

## Recursos computacionais ##

As análises presentes nesse artigo são todas realizadas no _software_ R
(versão `r with(R.version, paste0(major, ".", minor))`) e estão
disponíveis no sítio eletrônico do autor
<https://jreduardo.github.io/lce5859-mem/>.

# Resultados e discussão #

Após extração dos dados (\autoref{conjunto-de-dados}) realizou-se a
higienização dos textos que consistiu na remoção das palavras de parada
(preposições, artigos, conjunções, etc.), da pontuação, dos espaços em
branco e dos números. Além da remoção desses caracteres, realizou-se a
radicalização das palavras restantes utilizando o algoritmo de Poter
[@porter2001]. Do processo de higienização descrito, restaram `r
length(tm::Terms(dtm_texts))` palavras distintas em todos os textos.

Na \autoref{fig:wordcloud} são apresentados os 5\% termos, provenientes
da radicalização, mais frequentes em todos os textos. Note que os termos
mais frequentes são realmente àqueles utilizadas no ambiente de
Knowledge Discovery and Data Mining. Destaque para as palavras **model**
e **data**, o que reflete a característica do evento em discutir estudos
aplicados.

```{r wordcloud, out.width="0.7\\textwidth", fig.height=5, fig.width=12, fig.cap="Nuvem dos 5\\% termos mais frequentes nos textos extraídos dos resumos e títulos dos artigos apresentados no SIGKDD 2016."}

## Wordcloud
paleta <- brewer.pal(9, "Greys")[-(1:4)]
corte <- quantile(counts, probs = 0.95)
wnames <- names(counts)[counts > corte] %>%
    stringi::stri_trans_general(id = "latin-ascii")
cnames <- counts[counts > corte]
wordcloud(words = wnames,
          freq = cnames,
          min.freq = 1,
          random.order = FALSE,
          colors = paleta,
          family = "serif")

```

Os termos, provenientes da radicalização palavras, e os documentos,
nesse caso os artigos, formam uma matriz denominada matriz
termo-documento ($\mathbb{X}$). Nessa matriz as linhas são os artigos e
as colunas os termos, os valores representam a frequência do termo no
artigo. O gráfico à esquerda na \autoref{fig:pca-plot} ilustra essa
matriz, apenas com os 30 termos mais frequentes. Note que há muitos
valores iguais ou próximos a zero, mesmo dentre os 30 termos mais
frequentes, isso ocorre pois há muitos termos específicos dentro da área
de Knowledge Discovery e Data Mining.

```{r pca, cache=TRUE}

##----------------------------------------------------------------------
## Analysis

##-------------------------------------------
## Dimensionality reduction

## PCA decomposition
Vcor <- cov(mat_texts)
deco <- eigen(Vcor, symmetric = TRUE)
li <- deco$values
ei <- deco$vectors
pvar <- li/sum(li)

## Define the number of principal components
ni <- sum(cumsum(pvar) < 0.999)
scores <- mat_texts %*% ei[, 1:ni]

```

A dimensão da matriz $\mathbb{X}$ é `r nrow(mat_texts)` $\times$
`r ncol(mat_texts)`. Como o número de colunas é muito maior que o número
de linhas, qualquer análise de agrupamentos (que utilize distâncias)
ficará prejudicada devido a chamada "maldição da dimensionalidade"
[@Friedman2001]. Assim procedeu-se com a construção de componentes
principais para redução de dimensionalidade. No gráfico à direita da
\autoref{fig:pca-plot} é apresentada a evolução do percentual da
variância explicada pelo número de componentes principais. Note que com
`r ni` componentes já explica-se quase 100\% da varição total. Portanto,
foram tomadas `r ni` componentes, uma redução de `r ncol(mat_texts)-ni`
dimensões colunas da matriz $\mathbb{X}$.

```{r pca-plot, fig.height=4.5, fig.width=11, fig.cap="Representação da matriz termo-documento de frequências com os 50 termos mais frequentes (à esquerda) e proporção acumulada da variância explicada pelo número de componentes considerados (à direita)"}

## Visualizing data matrix
aux <- mat_texts[, order(counts, decreasing = TRUE)[1:30]]
xy1 <- levelplot(aux,
                 aspect = "fill",
                 col.regions = colorRampPalette(
                     c("gray90",  "gray50", "gray20"))(100),
                 ylab = "Termos",
                 xlab = "Artigos",
                 colorkey = list(space = "top"),
                 scales = list(
                     x = list(rot = 90, cex = 0.7, labels = NULL)),
                 par.settings = list(
                     layout.heights = list(
                         axis.xlab.padding = -2,
                         key.axis.padding = -1))
                 )

txt <- parse(text = paste0("lambda[", ni, "]==", li[ni]))
xy2 <- xyplot(cumsum(pvar)[1:250] ~ seq(pvar)[1:250],
              pch = 19, type = c("g", "l", "p"),
              xlab = "Nº de componentes",
              ylab = "% variância explicada",
              scales = list(y = list(rot = 90)),
              par.settings = list(
                  layout.widths = list(ylab.axis.padding = 0)),
              panel = function(x, y, ...) {
                  panel.xyplot(x, y, ...)
                  panel.abline(h = 0.99, v = ni, lty = 2)
                  panel.points(x[ni], y[ni], col = cols[2], pch = 19)
                  panel.text(x[ni] - 5, y[ni] - 0.06,
                             txt, cex = 0.9, pos = 4)
              })

print(xy1, position = c(0, 0, 0.62, 1), more = TRUE)
print(xy2, position = c(0.6, 0, 1, 1), more = FALSE)

```

```{r kmeans-gap, cache=TRUE}

##-------------------------------------------
## K-means clustering (choosen number of groups)

## Find groups
ks <- structure(2:15, names = paste0("k", 2:15))
lkms <- lapply(ks, function(k) {
    kmeans(x = scores, centers = k,
           iter.max = 50, nstart = 10)
})
meds <- sapply(lkms, function(x) {
    c("intra" = x$tot.withinss, "entre" = x$betweens)
})
meds <- cbind(t(meds), k = ks)

## Compute gap statistic
len <- length(lkms) - 1
mat <- matrix(nrow = len, ncol = 2)
for (u in 1:len) {
    cls <- cbind(lkms[[u]]$cluster, lkms[[u + 1]]$cluster)
    gap <- index.Gap(mat_texts, cls, B = 12, method = "k-means")
    mat[u, ] = do.call("c", gap)
}
colnames(mat) <- names(gap)
gaps <- cbind(k = 1:len + 1, mat)

```

```{r sqs-gap, fica.height=5, fig.width=11, fig.cap="Medidas de qualidade de agrupamento. Soma das distâncias euclidianas intra e entre clusters (esquerda) e diferenças de indíces Gap (direita)."}

## Illustrate statistics for choose the number of groups
xy1a <- xyplot(intra/1000 ~ k, pch = 19,
               type = "o", cex = 0.7,
               data = as.data.frame(meds),
               ylab = "SQ intra-clusters (em milhares)",
               scales = list(x = list(at = ks)))
xy1b <- xyplot(entre/1000 ~ k, pch = 19,
               type = "o", cex = 0.7,
               data = as.data.frame(meds),
               ylab = "SQ entre-clusters (em milhares)",
               scales = list(x = list(at = ks)))
xy1 <- doubleYScale(
    xy1a, xy1b, add.ylab2 = TRUE,
    text = c("Dentro dos clusters", "Entre os clusters"),
    title = "Soma de quadrados (SQ) das observações",
    cex.title = 1.1,
    points = TRUE,
    column = 2) +
    layer(panel.abline(v = ks, col = "gray80", lty = 2))
lab <- expression("Gap"*(k)-"Gap"*(k+1) +~s[k+1])
xy2 <- xyplot(diffu ~ k,
              data = as.data.frame(gaps),
              ylab = lab,
              type = c("p"),
              pch = 19,
              scales = list(y = list(rot = 90),
                            x = list(at = ks)),
              panel = function(x, y, subscripts, ... ) {
                  cols <- rep("gray60", length(y))
                  cols[y > 0] <- 1
                  panel.abline(v = ks, col = "gray80", lty = 2)
                  panel.xyplot(x, y, col = cols, ...)
                  panel.abline(h = 0, lty = 2, col = 1)
              })

print(xy1, position = c(0.0, 0.0, 0.52, 1.0), more = TRUE)
print(xy2, position = c(0.55, 0.0, 1.0, 0.93), more = FALSE)

## Choose k according to Tibishirani 2001
kchoosen <- with(as.data.frame(gaps), {
    min(k[diffu > 0])
})

```

Com as `r ni` componentes realizou-se o agrupamento não hierárquico
_k-means_, conforme algoritmo proposto por @Hartigan1979. Como
agrupamentos não hierárquicos necessitam que o número de grupos a serem
formados seja conhecido, os artigos foram agrupados em $2, 3, \ldots,
15$ grupos. Os resultados dos agrupamentos são apresentados na
\autoref{fig:sqs-gap}. No gráfico à esquerda são exibidas as somas de
quadrados $\sum_{i=j} (x_i - x_j)^2$ das observações alocadas em um
mesmo grupo ($SQ$ intra-cluster, em preto), e a soma de quadrados das
observações alocadas em um grupos diferentes ($SQ$ entre-cluster, em
azul). Para um bom agrupamento espera-se que a distância intra-cluster
seja baixa e a distância entre-cluster alta, porém essas são medidas
inversamente proporcionais e assim desejamos um bom compromisso entre as
duas medidas para a escolha do número de grupos. Embora as escalas sejam
distintas (valores dos eixos $y$), por esse gráfico, o número de grupos
adequado está em torno de 7 ou 8. Calculando os estatísticas Gap
[@Tibshirani2001] o número de grupos indicado é `r kchoosen`, o menor
$k$ em que a condição $Gap(k) - Gap(k+1) + s_{k+1} > 0$ (gráfico à
direita da \autoref{fig:sqs-gap}), o que está concordante com o gráfico
de distâncias intra e entre-clusters.

```{r}

##-------------------------------------------
## Verify groups
agrup <- lkms[[kchoosen-1]]
data_agrup <- cbind(data_short, "group" = factor(agrup$cluster))

split_results <- lapply(levels(data_agrup$group), function(g) {
    da <- subset(data_agrup, group == g)
    data_texts <- with(da, paste(title, abstract))
    dtm_texts <- text2dtm(data_texts)
    mat_texts <- as.matrix(dtm_texts)
    counts <- colSums(mat_texts)
    list("narticles" = nrow(da),
         "dtm_texts" = dtm_texts,
         "mat_texts" = mat_texts,
         "counts" = counts)
})

narticles <- sapply(split_results, function(x) x$narticles)

```

Note que o número de grupos estimado pelas estatísticas Gap, assim como
visto nas medidas de homogeneidade dos grupos e heterogeneidade entre
grupos, mostram que os `r length(levels(data_long$topic))` tópicos
criados pelos organizadores do evento são muito mais do que o necessário
para agrupar artigos similares.

Os `r kchoosen` grupos formados pelo algoritmo k-means contém
`r narticles` elementos (respectivamente). Para permitir a interpretação
dos grupos, exibi-se na \autoref{fig:wordcloud-groups} os 5\% termos
mais frequentes em cada grupo. Note que o Grupo `r which.max(narticles)`
é formado por um número de artigos muito maior que os demais, o que
indica o que consequentemente acarreta em um número maior de termos e
maior heterogeneidade do grupo. Isso fica claro ao observar a gama de
termos apresentadas para esse grupo na
\autoref{fig:wordcloud-groups}. A partir dessa figura também pode-se
caracterizar os grupos:

* **Grupo 1:** artigos essencialmente sobre agrupamentos;
* **Grupo 2:** artigos essencialmente sobre propostas de modelagem de
  dados;
* **Grupo 3:** artigos diversos sobre análise de dados para Knowledge
  Discovery e Data Mining;
* **Grupo 4:** artigos essencialmente sobre reconhecimento de padrão e
  e análise de grafos;
* **Grupo 5:** artigos relacionados a métodos de aprendizado de
  máquina^[o termo _feature_ em machine learning é geralmente usado
  como sinônimo para variáveis preditoras.]; e
* **Grupo 6:** artigos essencialmente sobre redes neurais.

```{r wordcloud-groups, fig.height=6, fig.width=12, fig.cap="Nuvem com os 5\\% termos mais frequentes em cada grupo formado pelo algoritmo k-means."}

## Wordclouds
par(mfrow = c(2, 3), mar = c(0, 0, 0, 0))
sapply(seq(unique(agrup$cluster)), function(i) {
    main <- paste0("Grupo ", i, " (", split_results[[i]]$narticles,
                   " artigos)")
    counts <- split_results[[i]]$counts
    paleta <- brewer.pal(9, "Greys")[-(1:4)]
    corte <- quantile(counts, probs = 0.95)
    wnames <- names(counts)[counts > corte] %>%
        stringi::stri_trans_general(id = "latin-ascii")
    cnames <- counts[counts > corte]
    wordcloud(words = wnames,
              freq = cnames,
              min.freq = 1,
              random.order = FALSE,
              colors = paleta,
              family = "serif")
    mtext(main, line = -1.5)
    box()
})

```

```{r}

## Random select 2 articles for each group
set.seed(1994)
index <- lapply(levels(data_agrup$group), function(i) {
    sample(which(data_agrup$group == i), 2)
})
random_da <- data_agrup[unlist(index), c("group", "title")]

```

Como resultado complementar, são apresentados na \autoref{tab:amostra}
os títulos de dois artigos escolhidos aleatoriamente de cada um dos
grupos formados. Pode se observar que os títulos apresentados condizem
com a representação dos grupos realizada na
\autoref{fig:wordcloud-groups}. Porém vale ressaltar que o título
somente uma pequena parte do conjunto de textos que caracterizam o
artigo, a maior parte da informação provém do resumo.

\begin{longtable}[c]{>{\arraybackslash}p{0.8cm}p{0.8cm}p{15cm}}
\caption{Amostras aleatórias de dois artigos em cada grupo}
\label{tab:amostra}\\[-0.3cm]
  \toprule
  Índice & Grupo & Título do artigo  \\
  \hline
```{r, results="asis"}
print(xtable(random_da),
      include.colnames = FALSE,
      hline.after = NULL,
      only.contents = TRUE)

```
  \bottomrule
\end{longtable}

# Conclusões #

```{r}

## Dimensions
npa <- nrow(data_long)
nar <- nrow(data_short)
nto <- length(levels(data_long$topic))
nx <- nrow(mat_texts)
px <- ncol(mat_texts)
mx <- ncol(scores)

```

Nesse artigo foram apresentados os procedimentos para extração de dados
textuais de páginas web, nomeadamente, os títulos e resumos dos artigos
apresentados na _22nd SIGKDD Conference_. Foram `r npa` páginas
consultadas e `r nar` artigos, cujo título e resumos foram
extraídos. Após pré-processamento os `r nar` artigos forneceram `r px`
termos distintos. A coleção de `r nar` artigos, organizada em uma matriz
termo-documento, de dimensão `r nx` $\times$ `r px` foi submetida a uma
análise de componentes principais onde `r ni` componentes foram retidas
para continuidade da análise, ou seja, a matriz termo-documento foi
reduzido de `r px` colunas para `r ni`. Com a nova matriz em que para
cada artigo foi associado valores de `r ni` componentes, realizou-se
análises de agrupamento por _k-means_ para $k$'s de 2
a 15. Verificou-se, pela estatística de Gap, que o número de grupos
adequado a esses dados são `r kchoosen`, muito menor do que os `r nto`
tópicos definidos pelo evento. A avaliação dos grupos mostrou
que o agrupamento conseguiu juntar artigos com conteúdos similares
podendo nomear esses grupos como
\vspace{-0.2cm}

* **Grupo 1:** artigos essencialmente sobre agrupamentos;
* **Grupo 2:** artigos essencialmente sobre propostas de modelagem de
  dados;
* **Grupo 3:** artigos diversos sobre análise de dados para Knowledge
  Discovery e Data Mining;
* **Grupo 4:** artigos essencialmente sobre reconhecimento de padrão e
  e análise de grafos;
* **Grupo 5:** artigos relacionados a métodos de aprendizado de máquina;
  e
* **Grupo 6:** artigos essencialmente sobre redes neurais.

Com base nos resultados obtidos fica evidente que uma abordagem
estatística ou heurística, para a definição de temas para trabalhos
apresentados em eventos científicos, leva a uma melhor organização dos
trabalhos, do que a atribuição subjetiva realizada pelos
organizadores. A aplicação da metodologia apresentada também não se
restringe a trabalhos acadêmicos, agrupamento de _posts_ em blogs, de
comentários em redes sociais, reclamações em central de atendimentos,
entre outros são exemplos em que as análises discutidas no artigo podem
ser replicadas com a devida adequação.

Como pesquisas decorrentes desse trabalho sugere-se uma abordagem
probabilística para definição dos temas, @Blei2012 apresenta alguns
modelos de probabilísticos de tópicos que podem auxiliar na formação dos
temas, em especial o modelo de alocação latente de Dirichlet parece
satisfazer o objetivo formulado nesse artigo.

\pagebreak

# Referências # {-}

\setlength\parindent{0pt}
\setlength{\parskip}{0.2cm}

\small
